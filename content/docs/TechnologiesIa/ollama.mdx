---
title: Intégration Ollama
description: Présentation de l'intégration Ollama dans la Plateforme d'Évaluation Automatisée d'Exercices de Bases de Données
---


## Qu'est-ce qu'Ollama?

[Ollama](https://ollama.ai) est une solution open-source qui permet d'exécuter des modèles de langage large (LLMs) localement sur votre propre matériel. Cette solution offre la possibilité de déployer des modèles d'IA comme Llama 2, Mistral, ou Gemma sans dépendre de services cloud.

<img  src="https://www.librechat.ai/images/blog/2024-03-02_ollama.png" className="w-50 rounded-lg"/>


## Caractéristiques principales d'Ollama

Ollama propose plusieurs fonctionnalités clés:

- **Exécution locale**: Les modèles s'exécutent entièrement sur votre propre matériel
- **Confidentialité des données**: Aucune donnée n'est envoyée à des serveurs externes
- **Personnalisation**: Possibilité d'ajuster et de fine-tuner les modèles selon vos besoins
- **API simple**: Interface REST facile à intégrer dans des applications existantes
- **Support multi-modèles**: Accès à une bibliothèque croissante de modèles pré-entraînés
- **Faible empreinte**: Optimisé pour fonctionner efficacement sur du matériel standard

[//]: # ()
[//]: # (## Architecture d'intégration prévue)

[//]: # ()
[//]: # (Notre plan initial d'intégration d'Ollama prévoyait l'architecture suivante:)

[//]: # ()
[//]: # (![Architecture d'intégration Ollama]&#40;/images/integrations/ollama-architecture.png&#41;)

### Endpoints développés

Nous avons développé plusieurs endpoints pour communiquer avec Ollama:

```javascript
// Exemple d'endpoint pour l'évaluation de code SQL
app.post('/api/evaluate-sql', async (req, res) => {
  try {
    const { studentQuery, referenceQuery, database } = req.body;

    // Envoi à l'API Ollama
    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: "mistral",
        prompt: `Évalue cette requête SQL d'étudiant: ${studentQuery} par rapport à la requête de référence: ${referenceQuery} sur la base de données ${database}.`
      })
    });

    const data = await response.json();
    res.json({ evaluation: data.response });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
```

## Limitations et défis rencontrés

Malgré les avantages potentiels d'Ollama, nous avons rencontré plusieurs défis qui ont finalement conduit à l'abandon de cette solution:

### 1. Contraintes de performance

L'exécution locale des modèles nécessitait des ressources matérielles importantes:
- **GPU dédié**: Nécessaire pour des performances acceptables
- **Latence élevée**: Temps de réponse trop long sur des machines standard
- **Limites de parallélisation**: Difficulté à gérer plusieurs requêtes simultanées

### 2. Environnement local

L'aspect local d'Ollama présentait des contraintes opérationnelles:
- **Absence de VPX/VPS**: Pas d'environnement serveur dédié disponible
- **Difficultés de déploiement**: Complexité pour déployer la solution à l'échelle
- **Maintenance**: Nécessité de gérer les mises à jour et la disponibilité du service

### 3. Consistance des résultats

Nos tests ont révélé des problèmes de consistance:
- **Variabilité des évaluations**: Résultats parfois incohérents entre les différentes exécutions
- **Qualité d'évaluation**: Précision insuffisante pour certains types d'exercices complexes

## Conclusion et transition

Bien qu'Ollama représente une solution prometteuse pour l'exécution locale de modèles d'IA, les contraintes identifiées nous ont amenés à rechercher une alternative plus adaptée à nos besoins de production.

Cette recherche nous a conduits vers [OpenRouter](/docs/TechnologiesIa/openrouter), une solution qui offre un accès unifié à divers modèles d'IA hébergés dans le cloud, avec des performances et une disponibilité supérieures.

---

**Section connexe**: [Intégration OpenRouter](/docs/TechnologiesIa/openrouter)